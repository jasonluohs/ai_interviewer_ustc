# -*- coding: utf-8 -*-
"""
AI é¢è¯•æŠ¥å‘Šç”Ÿæˆæ¨¡å—
ä½¿ç”¨é˜¿é‡Œäº‘ DashScope API è°ƒç”¨ Qwen-max (æ€è€ƒæ¨¡å¼) å¯¹é¢è¯•å¯¹è¯è¿›è¡Œè¯„ä»·ã€‚
"""

from typing import List, Dict, Optional
from openai import OpenAI

from config import DASHSCOPE_API_KEY, LLM_BASE_URL

# ==================== å®¢æˆ·ç«¯åˆå§‹åŒ– ====================
client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url=LLM_BASE_URL,  # https://dashscope.aliyuncs.com/compatible-mode/v1
)

# ==================== è¯„ä»·ç”¨ç³»ç»Ÿæç¤ºè¯ ====================
REPORT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æŠ€æœ¯é¢è¯•è¯„å®¡ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸€æ®µå®Œæ•´çš„é¢è¯•å¯¹è¯è®°å½•ï¼Œå¯¹è¢«é¢è¯•è€…çš„è¡¨ç°è¿›è¡Œå…¨é¢ã€å®¢è§‚ã€ä¸“ä¸šçš„è¯„ä»·ã€‚

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œè¯„ä»·ï¼Œå¹¶ç»™å‡ºæ€»ä½“è¯„åˆ†ï¼ˆæ»¡åˆ†100åˆ†ï¼‰ï¼š

## è¯„ä»·ç»´åº¦

1. **æŠ€æœ¯èƒ½åŠ›**ï¼ˆæƒé‡ 30%ï¼‰
   - å¯¹æŠ€æœ¯é—®é¢˜çš„ç†è§£æ·±åº¦
   - çŸ¥è¯†é¢çš„å¹¿åº¦
   - æ˜¯å¦èƒ½å‡†ç¡®è¿ç”¨æŠ€æœ¯æ¦‚å¿µ

2. **é—®é¢˜è§£å†³èƒ½åŠ›**ï¼ˆæƒé‡ 25%ï¼‰
   - åˆ†æé—®é¢˜çš„é€»è¾‘æ€§
   - è§£é¢˜æ€è·¯æ˜¯å¦æ¸…æ™°
   - èƒ½å¦æå‡ºæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆ

3. **æ²Ÿé€šè¡¨è¾¾èƒ½åŠ›**ï¼ˆæƒé‡ 20%ï¼‰
   - å›ç­”æ˜¯å¦æ¡ç†æ¸…æ™°
   - èƒ½å¦ç®€æ´å‡†ç¡®åœ°è¡¨è¾¾è§‚ç‚¹
   - æ˜¯å¦å–„äºç”¨ä¾‹å­è¯´æ˜é—®é¢˜

4. **å­¦ä¹ æ½œåŠ›ä¸æ€ç»´æ·±åº¦**ï¼ˆæƒé‡ 15%ï¼‰
   - é¢å¯¹ä¸ç†Ÿæ‚‰çš„é—®é¢˜æ˜¯å¦èƒ½åˆç†æ¨å¯¼
   - æ˜¯å¦å±•ç°å‡ºä¸¾ä¸€åä¸‰çš„èƒ½åŠ›
   - æ€è€ƒæ˜¯å¦æœ‰æ·±åº¦

5. **ç»¼åˆç´ å…»**ï¼ˆæƒé‡ 10%ï¼‰
   - é¢è¯•æ€åº¦ä¸è‡ªä¿¡ç¨‹åº¦
   - é¢å¯¹å‹åŠ›çš„è¡¨ç°
   - å›ç­”çš„å®Œæ•´æ€§

## è¾“å‡ºæ ¼å¼è¦æ±‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºè¯„ä»·æŠ¥å‘Šï¼š

---
# ğŸ“‹ AI é¢è¯•è¯„ä»·æŠ¥å‘Š

## ä¸€ã€æ€»ä½“è¯„åˆ†ï¼šXX / 100

## äºŒã€å„ç»´åº¦è¯¦ç»†è¯„ä»·

### 1. æŠ€æœ¯èƒ½åŠ›ï¼ˆXX / 30ï¼‰
[å…·ä½“è¯„ä»·å†…å®¹]

### 2. é—®é¢˜è§£å†³èƒ½åŠ›ï¼ˆXX / 25ï¼‰
[å…·ä½“è¯„ä»·å†…å®¹]

### 3. æ²Ÿé€šè¡¨è¾¾èƒ½åŠ›ï¼ˆXX / 20ï¼‰
[å…·ä½“è¯„ä»·å†…å®¹]

### 4. å­¦ä¹ æ½œåŠ›ä¸æ€ç»´æ·±åº¦ï¼ˆXX / 15ï¼‰
[å…·ä½“è¯„ä»·å†…å®¹]

### 5. ç»¼åˆç´ å…»ï¼ˆXX / 10ï¼‰
[å…·ä½“è¯„ä»·å†…å®¹]

## ä¸‰ã€äº®ç‚¹æ€»ç»“
[è¢«é¢è¯•è€…è¡¨ç°çªå‡ºçš„åœ°æ–¹]

## å››ã€æ”¹è¿›å»ºè®®
[éœ€è¦åŠ å¼ºå’Œæ”¹è¿›çš„åœ°æ–¹]

## äº”ã€æ€»ä½“è¯„è¯­
[ä¸€æ®µç®€æ´çš„æ€»ç»“æ€§è¯„è¯­]
---

è¯·ç¡®ä¿è¯„ä»·å®¢è§‚ã€å…¬æ­£ï¼Œæ—¢è¦è‚¯å®šä¼˜ç‚¹ä¹Ÿè¦æŒ‡å‡ºä¸è¶³ï¼Œç»™å‡ºæœ‰å»ºè®¾æ€§çš„åé¦ˆã€‚"""


def _format_history_for_report(history: List[Dict[str, str]]) -> str:
    """
    å°†å¯¹è¯å†å²æ ¼å¼åŒ–ä¸ºé¢è¯•å¯¹è¯è®°å½•æ–‡æœ¬ï¼Œä¾›è¯„ä»·æ¨¡å‹é˜…è¯»ã€‚

    å‚æ•°:
        history: å¯¹è¯å†å²åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º {"role": "user"|"assistant", "content": "..."}

    è¿”å›:
        æ ¼å¼åŒ–åçš„å¯¹è¯è®°å½•å­—ç¬¦ä¸²
    """
    if not history:
        return "ï¼ˆæ— å¯¹è¯è®°å½•ï¼‰"

    lines = []
    turn = 0
    for msg in history:
        role = msg.get("role", "")
        content = msg.get("content", "").strip()
        if not content:
            continue
        if role == "user":
            turn += 1
            lines.append(f"ã€ç¬¬ {turn} è½®ã€‘")
            lines.append(f"å€™é€‰äººï¼š{content}")
        elif role == "assistant":
            lines.append(f"é¢è¯•å®˜ï¼š{content}")
        lines.append("")  # ç©ºè¡Œåˆ†éš”

    return "\n".join(lines)


def ai_report(
    history: List[Dict[str, str]],
    model: str = "qwen-max",
    enable_thinking: bool = True,
) -> str:
    """
    æ ¹æ®å®Œæ•´å¯¹è¯å†å²ï¼Œè°ƒç”¨é˜¿é‡Œäº‘ Qwen-max (æ€è€ƒæ¨¡å¼) ç”Ÿæˆé¢è¯•è¯„ä»·æŠ¥å‘Šã€‚

    å‚æ•°:
        history:  å®Œæ•´çš„é¢è¯•å¯¹è¯å†å²åˆ—è¡¨
                  æ ¼å¼: [{"role": "user"|"assistant", "content": "..."}, ...]
                  å…¶ä¸­ role="user" æ˜¯è¢«é¢è¯•è€…çš„å›ç­”ï¼Œrole="assistant" æ˜¯é¢è¯•å®˜çš„æé—®/è¿½é—®ã€‚
        model:    ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ "qwen-max"ï¼ˆæ€è€ƒæ¨¡å¼ä¸‹å®é™…ä¼šèµ° qwen-max çš„æ·±åº¦æ¨ç†ï¼‰
        enable_thinking: æ˜¯å¦å¼€å¯æ€è€ƒæ¨¡å¼ï¼ˆæ·±åº¦æ¨ç†ï¼‰ï¼Œé»˜è®¤å¼€å¯

    è¿”å›:
        str: AI ç”Ÿæˆçš„é¢è¯•è¯„ä»·æŠ¥å‘Šæ–‡æœ¬

    å¼‚å¸¸:
        å¦‚æœ API è°ƒç”¨å¤±è´¥ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸ã€‚

    ä½¿ç”¨ç¤ºä¾‹:
        >>> from modules.ai_report import ai_report
        >>> history = [
        ...     {"role": "assistant", "content": "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»"},
        ...     {"role": "user", "content": "æˆ‘æ˜¯XXXï¼Œæœ¬ç§‘è®¡ç®—æœºç§‘å­¦..."},
        ...     {"role": "assistant", "content": "è¯·ä»‹ç»ä¸€ä¸‹TCPä¸‰æ¬¡æ¡æ‰‹"},
        ...     {"role": "user", "content": "TCPä¸‰æ¬¡æ¡æ‰‹æ˜¯..."},
        ... ]
        >>> report = ai_report(history)
        >>> print(report)
    """
    if not history:
        return "âš ï¸ æ²¡æœ‰å¯¹è¯è®°å½•ï¼Œæ— æ³•ç”Ÿæˆé¢è¯•è¯„ä»·æŠ¥å‘Šã€‚"

    # æ ¼å¼åŒ–å¯¹è¯è®°å½•
    formatted_history = _format_history_for_report(history)

    # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
    user_turns = sum(1 for msg in history if msg.get("role") == "user")
    assistant_turns = sum(1 for msg in history if msg.get("role") == "assistant")

    # æ„é€ ç”¨æˆ·æ¶ˆæ¯ï¼šæŠŠæ•´æ®µé¢è¯•å¯¹è¯äº¤ç»™è¯„ä»·æ¨¡å‹
    user_message = (
        f"ä»¥ä¸‹æ˜¯ä¸€æ®µå®Œæ•´çš„æŠ€æœ¯é¢è¯•å¯¹è¯è®°å½•ï¼Œå…± {user_turns} è½®å€™é€‰äººå›ç­”ã€"
        f"{assistant_turns} è½®é¢è¯•å®˜æé—®ã€‚\n"
        f"è¯·æ ¹æ®å¯¹è¯å†…å®¹å¯¹è¢«é¢è¯•è€…çš„è¡¨ç°è¿›è¡Œå…¨é¢è¯„ä»·ã€‚\n\n"
        f"--- é¢è¯•å¯¹è¯è®°å½• ---\n\n"
        f"{formatted_history}\n"
        f"--- å¯¹è¯è®°å½•ç»“æŸ ---\n\n"
        f"è¯·æŒ‰è¦æ±‚çš„æ ¼å¼è¾“å‡ºè¯„ä»·æŠ¥å‘Šã€‚"
    )

    messages = [
        {"role": "system", "content": REPORT_SYSTEM_PROMPT},
        {"role": "user", "content": user_message},
    ]

    try:
        # æ„é€ è¯·æ±‚å‚æ•°
        request_params = {
            "model": model,
            "messages": messages,
        }

        # å¼€å¯æ€è€ƒæ¨¡å¼ï¼ˆQwen-max æ·±åº¦æ¨ç†ï¼‰
        # DashScope å…¼å®¹æ¨¡å¼ä¸‹ï¼Œé€šè¿‡ extra_body ä¼ é€’ enable_thinking å‚æ•°
        if enable_thinking:
            request_params["extra_body"] = {"enable_thinking": True}

        completion = client.chat.completions.create(**request_params)

        # æå–å›å¤å†…å®¹
        if completion.choices and completion.choices[0].message:
            return completion.choices[0].message.content or "âš ï¸ æ¨¡å‹æœªè¿”å›æœ‰æ•ˆå†…å®¹ã€‚"
        else:
            return "âš ï¸ æ¨¡å‹è¿”å›ä¸ºç©ºï¼Œè¯·ç¨åé‡è¯•ã€‚"

    except Exception as e:
        raise RuntimeError(f"é¢è¯•è¯„ä»·æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}") from e


def ai_report_stream(
    history: List[Dict[str, str]],
    model: str = "qwen-max",
    enable_thinking: bool = True,
):
    """
    æµå¼ç‰ˆæœ¬ï¼šæ ¹æ®å®Œæ•´å¯¹è¯å†å²ï¼Œæµå¼ç”Ÿæˆé¢è¯•è¯„ä»·æŠ¥å‘Šã€‚
    é€‚ç”¨äº Streamlit ç­‰éœ€è¦é€æ­¥æ˜¾ç¤ºå†…å®¹çš„å‰ç«¯ã€‚

    å‚æ•°:
        history:  å®Œæ•´çš„é¢è¯•å¯¹è¯å†å²åˆ—è¡¨
        model:    ä½¿ç”¨çš„æ¨¡å‹åç§°
        enable_thinking: æ˜¯å¦å¼€å¯æ€è€ƒæ¨¡å¼

    è¿”å›:
        Generator[str]: é€æ­¥ç´¯ç§¯çš„æŠ¥å‘Šæ–‡æœ¬ï¼ˆæ¯æ¬¡ yield åŒ…å«ä»å¼€å¤´åˆ°å½“å‰çš„å®Œæ•´æ–‡æœ¬ï¼‰

    ä½¿ç”¨ç¤ºä¾‹ï¼ˆStreamlit é›†æˆï¼‰:
        >>> placeholder = st.empty()
        >>> for partial_report in ai_report_stream(st.session_state.history):
        ...     placeholder.markdown(partial_report)
    """
    if not history:
        yield "âš ï¸ æ²¡æœ‰å¯¹è¯è®°å½•ï¼Œæ— æ³•ç”Ÿæˆé¢è¯•è¯„ä»·æŠ¥å‘Šã€‚"
        return

    # æ ¼å¼åŒ–å¯¹è¯è®°å½•
    formatted_history = _format_history_for_report(history)

    user_turns = sum(1 for msg in history if msg.get("role") == "user")
    assistant_turns = sum(1 for msg in history if msg.get("role") == "assistant")

    user_message = (
        f"ä»¥ä¸‹æ˜¯ä¸€æ®µå®Œæ•´çš„æŠ€æœ¯é¢è¯•å¯¹è¯è®°å½•ï¼Œå…± {user_turns} è½®å€™é€‰äººå›ç­”ã€"
        f"{assistant_turns} è½®é¢è¯•å®˜æé—®ã€‚\n"
        f"è¯·æ ¹æ®å¯¹è¯å†…å®¹å¯¹è¢«é¢è¯•è€…çš„è¡¨ç°è¿›è¡Œå…¨é¢è¯„ä»·ã€‚\n\n"
        f"--- é¢è¯•å¯¹è¯è®°å½• ---\n\n"
        f"{formatted_history}\n"
        f"--- å¯¹è¯è®°å½•ç»“æŸ ---\n\n"
        f"è¯·æŒ‰è¦æ±‚çš„æ ¼å¼è¾“å‡ºè¯„ä»·æŠ¥å‘Šã€‚"
    )

    messages = [
        {"role": "system", "content": REPORT_SYSTEM_PROMPT},
        {"role": "user", "content": user_message},
    ]

    try:
        request_params = {
            "model": model,
            "messages": messages,
            "stream": True,
        }

        if enable_thinking:
            request_params["extra_body"] = {"enable_thinking": True}
            # æµå¼ + æ€è€ƒæ¨¡å¼ä¸‹ï¼Œéœ€è¦è®¾ç½® stream_options ä»¥è·å–å¢é‡è¾“å‡º
            request_params["stream_options"] = {"include_usage": True}

        completion = client.chat.completions.create(**request_params)

        full_response = ""
        for chunk in completion:
            if chunk.choices and chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                yield full_response

    except Exception as e:
        yield f"âš ï¸ é¢è¯•è¯„ä»·æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
