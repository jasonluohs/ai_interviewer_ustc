from llm_agent import llm_stream_chat
import asyncio
from audio_processor import voice_to_text
VOICE_ENABLED = True

# é…ç½®
finance_interviewer = """æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èé¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•ä¸€ä½åº”è˜é‡‘èåˆ†æå¸ˆçš„å€™é€‰äººã€‚
è¯·æ ¹æ®é‡‘èè¡Œä¸šçš„ä¸“ä¸šçŸ¥è¯†ï¼Œæå‡ºç›¸å…³çš„é—®é¢˜ï¼Œå¹¶è¯„ä¼°å€™é€‰äººçš„å›ç­”ã€‚
é¢è¯•åº”åŒ…å«ä»¥ä¸‹æ–¹é¢ï¼š
1. é‡‘èåŸºç¡€çŸ¥è¯†
2. æ•°æ®åˆ†æèƒ½åŠ›
3. å¸‚åœºç†è§£
4. é£é™©æ„è¯†
5. èŒä¸šè§„åˆ’
"""from modules.llm_agent import llm_stream_chat

è¯·ä¸€æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜ï¼Œç­‰å¾…å€™é€‰äººå›ç­”åå†ç»§ç»­ã€‚
ä¿æŒä¸“ä¸šä½†å‹å¥½çš„æ€åº¦ã€‚
å½“é¢è¯•è¿›è¡Œåˆ°ç¬¬2ä¸ªé—®é¢˜æ—¶ï¼Œå¯ä»¥è‡ªç„¶ç»“æŸé¢è¯•ã€‚"""

history = [{'role': 'system', 'content': finance_interviewer}]
STEPFUN_API_KEY = "3ZrwQrJ6sG8i2AhNs89yejHYABzGnlT6pMpXaVxr1UDb4iSOQBeRzMwotRFXo3vP7"

# é¢è¯•çŠ¶æ€
interview_state = {
    'in_progress': False,
    'question_count': 0,
    'max_questions': 2
}

async def simple_voice_test():
    """è¯­éŸ³è¯†åˆ«"""
    try:
        text = await voice_to_text(STEPFUN_API_KEY)
        
        if text:
            print(f"\nâœ… è¯†åˆ«æˆåŠŸ!")
            print(f"ğŸ“ æ‚¨è¯´: {text}")
            return text
        else:
            print("\nâŒ è¯†åˆ«å¤±è´¥ - æœªæ£€æµ‹åˆ°è¯­éŸ³")
            return None
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ å½•éŸ³å·²åœæ­¢")
        return None
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        return None


async def chat_with_ai(user_input: str):
    """ä¸AIå¯¹è¯"""
    print("\nğŸ¤– AIé¢è¯•å®˜: ", end="", flush=True)
    
    full_response = ""
    try:
        for chunk in llm_stream_chat(history, user_input):
            if full_response:
                new_content = chunk[len(full_response):]
            else:
                new_content = chunk
            print(new_content, end="", flush=True)
            full_response = chunk
        
        # è®°å½•å¯¹è¯å†å²
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": full_response})
        
        # å¢åŠ é—®é¢˜è®¡æ•°
        if full_response and full_response.strip().endswith('?'):
            interview_state['question_count'] += 1
            print(f"\n\nğŸ“Š é—®é¢˜è¿›åº¦: {interview_state['question_count']}/{interview_state['max_questions']}")
        
        print()
        return full_response
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        return None


async def start_interview():
    """å¼€å§‹é¢è¯•"""
    print("\nğŸ¯ æ­£åœ¨å¯åŠ¨AIé¢è¯•å®˜...")
    
    # é‡ç½®çŠ¶æ€
    interview_state['in_progress'] = True
    interview_state['question_count'] = 0
    # AIæå‡ºç¬¬ä¸€ä¸ªé—®é¢˜
    await chat_with_ai("è¯·å¼€å§‹é¢è¯•ï¼Œæå‡ºç¬¬ä¸€ä¸ªé—®é¢˜ã€‚")


async def conduct_interview():
    """è¿›è¡Œé¢è¯•ä¸»å¾ªç¯"""
    if not interview_state['in_progress']:
        await start_interview()
    
    while interview_state['in_progress']:
        print("\n" + "-"*40)
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é—®é¢˜æ•°
        if interview_state['question_count'] >= 1:
            print("\nâœ… é¢è¯•é—®é¢˜å·²å®Œæˆ")
            await end_interview()
            break
        
        # ç”¨æˆ·è¯­éŸ³å›ç­”
        user_input = await simple_voice_test()
        
        if user_input is None:
            print("âš  æœªè¯†åˆ«åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•")
            continue
        await chat_with_ai(user_input)
        
        # ç®€çŸ­å»¶è¿Ÿ
        await asyncio.sleep(1)


async def end_interview():
    """ç»“æŸé¢è¯•"""
    interview_state['in_progress'] = False
    
    print("\n" + "="*50)
    print("ğŸ“‹ é¢è¯•æ€»ç»“")
    print(f"æ€»é—®é¢˜æ•°: {interview_state['question_count']}")
    
    # AIæä¾›æ€»ç»“
    print("\nğŸ¤– AIé¢è¯•å®˜: ", end="", flush=True)
    
    summary_prompt = "é¢è¯•å·²å®Œæˆï¼Œè¯·å¯¹å€™é€‰äººçš„æ•´ä½“è¡¨ç°è¿›è¡Œç®€è¦æ€»ç»“ï¼Œä¸è¦è¿åˆä»–ï¼Œæ‰¹åˆ¤æ€§æ€ç»´ï¼Œå¹¶æä¾›ä¸€äº›æ”¹è¿›å»ºè®®ã€‚"
    await chat_with_ai(summary_prompt)
    
    print("\nğŸ¯ é¢è¯•æµç¨‹ç»“æŸã€‚æ„Ÿè°¢æ‚¨çš„å‚ä¸ï¼")
    print("è¾“å…¥ 's' é‡æ–°å¼€å§‹é¢è¯•ï¼Œ'q' é€€å‡ºç¨‹åº")


async def main():
    """ä¸»ç¨‹åº"""
    while True:
        print("\né€‰æ‹©æ“ä½œ:")
        print("  [s] å¼€å§‹/ç»§ç»­é¢è¯•")
        print("  [q] é€€å‡ºç¨‹åº")
        
        choice = input("è¯·é€‰æ‹© (s/q): ").strip().lower()
        
        if choice == 'q':
            print("\nğŸ‘‹ å†è§!")
            break
        
        if choice == 's':
            if interview_state['in_progress']:
                print("âœ… ç»§ç»­é¢è¯•...")
            else:
                print("ğŸ¬ å¼€å§‹æ–°çš„é¢è¯•...")
            
            await conduct_interview()
      

    response=llm_stream_chat(history,user_input)
    print(f'AI:{next(response)}')"""

# app.py (Terminal æµ‹è¯•ç‰ˆ)
from modules.llm_agent import llm_stream_chat

# æ¨¡æ‹Ÿé…ç½®åŠ è½½
finance_interviewer = "æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„â€˜é‡‘èé¢è¯•å®˜â€™ã€‚æ‚¨çš„ç›®æ ‡æ˜¯æ¨¡æ‹ŸçœŸå®çš„é‡‘èè¡Œä¸šé¢è¯•åœºæ™¯ï¼Œè¯„ä¼°å€™é€‰äººçš„ä¸“ä¸šçŸ¥è¯†ã€é€»è¾‘æ€ç»´ã€é£é™©æ„è¯†å’ŒèŒä¸šç´ å…»ã€‚\nå®—æ—¨ä¸ç›®æ ‡ï¼š\n* ä¸ºç”¨æˆ·æä¾›é«˜åº¦ä¸“ä¸šä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„é‡‘èé¢è¯•ä½“éªŒã€‚\n* è¦†ç›–é‡‘èçš„ä¸åŒç»†åˆ†é¢†åŸŸï¼Œå¦‚æŠ•èµ„é“¶è¡Œã€èµ„äº§ç®¡ç†ã€é£é™©æ§åˆ¶ã€å®šé‡åˆ†æç­‰ã€‚\n* é’ˆå¯¹ç”¨æˆ·çš„å›ç­”æä¾›æ·±å…¥çš„åé¦ˆå’Œæ”¹è¿›å»ºè®®ã€‚\nè¡Œä¸ºä¸è§„åˆ™ï¼š\n1) é¢è¯•å‡†å¤‡ä¸å¼€åœºï¼š\na) çƒ­æƒ…ä¸”ä¸“ä¸šåœ°å‘å€™é€‰äººæ‰“æ‹›å‘¼ï¼Œä»‹ç»è‡ªå·±çš„èº«ä»½ä¸ºâ€˜é‡‘èé¢è¯•å®˜â€™ã€‚\nb) è¯¢é—®å€™é€‰äººæ‰€ç”³è¯·çš„å…·ä½“å²—ä½ï¼ˆä¾‹å¦‚ï¼šåˆ†æå¸ˆã€åŸºé‡‘ç»ç†ã€é£æ§ä¸“å‘˜ï¼‰ä»¥åŠå…¶èƒŒæ™¯ï¼ˆåº”å±Šç”Ÿæˆ–ç¤¾æ‹›ï¼‰ã€‚\nc) æ ¹æ®å²—ä½è®¾å®šé¢è¯•æµç¨‹ï¼Œé€šå¸¸åŒ…å«è‡ªæˆ‘ä»‹ç»ã€ä¸“ä¸šæŠ€æœ¯é—®é¢˜ï¼ˆTechnical Questionsï¼‰å’Œè¡Œä¸ºé¢è¯•é—®é¢˜ï¼ˆBehavioral Questionsï¼‰ã€‚\n2) é¢è¯•å®æ–½ï¼š\na) æå‡ºä¸å²—ä½ç´§å¯†ç›¸å…³çš„æŠ€æœ¯æ€§é—®é¢˜ï¼Œä¾‹å¦‚ä¼°å€¼æ¨¡å‹ï¼ˆDCFï¼‰ã€è´¢åŠ¡æŠ¥è¡¨åˆ†æã€å®è§‚ç»æµå½±å“æˆ–å¸‚åœºè¡ç”Ÿå“å®šä»·ã€‚\nb) é‡‡ç”¨â€˜å‹åŠ›é¢è¯•â€™æˆ–â€˜è¿½é—®â€™æ¨¡å¼ï¼Œåœ¨ç”¨æˆ·å›ç­”åï¼Œé’ˆå¯¹å…¶é€»è¾‘æ¼æ´è¿›ä¸€æ­¥æé—®ã€‚\nc) åŒ…å«è¡Œä¸ºé¢è¯•é¢˜ï¼Œä½¿ç”¨STARæ³•åˆ™ï¼ˆæƒ…å¢ƒã€ä»»åŠ¡ã€è¡ŒåŠ¨ã€ç»“æœï¼‰æ¥è¯„ä¼°ç”¨æˆ·çš„è½¯æŠ€èƒ½ã€‚\nd) æ¯æ¬¡å¯¹è¯ä»…æå‡º 1-2 ä¸ªé—®é¢˜ï¼Œä¿æŒèŠ‚å¥ï¼Œè®©è¿‡ç¨‹æ›´åƒçœŸå®å¯¹è¯ã€‚\n3) åé¦ˆä¸æ€»ç»“ï¼š\na) åœ¨é¢è¯•ç»“æŸæ—¶ï¼Œè¯¢é—®ç”¨æˆ·æ˜¯å¦éœ€è¦å¤ç›˜åé¦ˆã€‚\nb) æä¾›å…·ä½“çš„åé¦ˆï¼ŒåŒ…æ‹¬ä¸“ä¸šçŸ¥è¯†çš„å‡†ç¡®æ€§ã€è¡¨è¾¾çš„é€»è¾‘æ€§ä»¥åŠéœ€è¦åŠ å¼ºçš„é¢†åŸŸã€‚\næ•´ä½“è¯­æ°”ï¼š\n* è¯­æ°”æ­£å¼ã€å®¢è§‚ã€ä¸¥è°¨ï¼Œæœ‰æ—¶å¯ä»¥è¡¨ç°å‡ºé¢è¯•å®˜çš„å¨ä¸¥æ„Ÿã€‚\n * å¯¹é‡‘èä¸“ä¸šæœ¯è¯­ä½¿ç”¨ç²¾å‡†ã€‚\n* ä¿æŒé«˜æ•ˆï¼Œé¿å…å†—é•¿çš„åºŸè¯ã€‚" 
history = [{'role': 'system', 'content': finance_interviewer}]


while True:
    user_input = input("\nç”¨æˆ·: ")
    if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
        break
    
    print("AI: ", end="", flush=True)
    
    # full_responseå‰é¢çš„å…¨æ–‡
    full_response = ""
    for chunk in llm_stream_chat(history, user_input):
        # llm_stream_chat è¿”å›çš„æ˜¯ç´¯ç§¯æ–‡æœ¬ï¼Œè¿™é‡Œè®¡ç®—å¢é‡æ¥æ‰“å°
        # æˆ–è€…ä¹Ÿå¯ä»¥åœ¨ llm_agent.py é‡Œåª yield å¢é‡ã€‚
        # llm_agent.pyï¼ˆyield full_responseï¼‰
        new_content = chunk[len(full_response):] 
        print(new_content, end="", flush=True)
        full_response = chunk
    
    # æœ€åå°†å®Œæ•´å›ç­”å­˜å…¥ historyï¼Œç»´æŒå¯¹è¯ä¸Šä¸‹æ–‡
    history.append({"role": "user", "content": user_input})
    history.append({"role": "assistant", "content": full_response})
    print() # æ¢è¡Œ

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")
