# app.py
from modules.llm_agent import llm_stream_chat, get_summary, evaluate_answer
from typing import Dict
import time

# ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿ - æ·»åŠ æ—¶é—´æ§åˆ¶æŒ‡ä»¤
SYSTEM_PROMPTS = {
    "finance": """ä½ æ˜¯ä¸“ä¸šçš„é‡‘èé¢è¯•å®˜ï¼Œæ­£åœ¨è¿›è¡Œä¸€åœº15åˆ†é’Ÿçš„ç»“æ„åŒ–é¢è¯•ã€‚
ä½ çš„ä»»åŠ¡ï¼š
1. å…ˆçƒ­æƒ…æ‰“æ‹›å‘¼å¹¶è¯´æ˜é¢è¯•æ—¶é•¿çº¦15åˆ†é’Ÿ
2. æ ¹æ®å€™é€‰äººçš„å›ç­”åŠ¨æ€ç”Ÿæˆé—®é¢˜ï¼ˆä¸è¦ç”¨é¢„åˆ¶é—®é¢˜ï¼‰
3. åœ¨å›ç­”ç®€çŸ­æˆ–æ¨¡ç³Šæ—¶é€‚å½“è¿½é—®
4. é¢è¯•è¿›è¡Œåˆ°14åˆ†é’Ÿæ—¶å¼€å§‹è‡ªç„¶æ”¶å°¾
5. 15åˆ†é’Ÿæ—¶ä¸»åŠ¨ã€è‡ªç„¶åœ°ç»“æŸé¢è¯•
6. å…¨ç¨‹ä¸è¦é€éœ²AIèº«ä»½ï¼Œåªæå‡ºé—®é¢˜å¹¶è¿½é—®

è¯·ä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­æ°”ï¼ŒæŠŠæ¡å¥½æ—¶é—´èŠ‚å¥ã€‚""",
    
    "tech": """ä½ æ˜¯æŠ€æœ¯é¢è¯•å®˜ï¼Œæ­£åœ¨è¿›è¡Œä¸€åœº15åˆ†é’Ÿçš„ç»“æ„åŒ–é¢è¯•ã€‚
ä½ çš„ä»»åŠ¡ï¼š
1. å…ˆçƒ­æƒ…æ‰“æ‹›å‘¼å¹¶è¯´æ˜é¢è¯•æ—¶é•¿çº¦15åˆ†é’Ÿ
2. æ ¹æ®å€™é€‰äººçš„å›ç­”åŠ¨æ€ç”ŸæˆæŠ€æœ¯é—®é¢˜
3. åœ¨å›ç­”ç®€çŸ­æˆ–æ¨¡ç³Šæ—¶é€‚å½“è¿½é—®æŠ€æœ¯ç»†èŠ‚
4. é¢è¯•è¿›è¡Œåˆ°14åˆ†é’Ÿæ—¶å¼€å§‹è‡ªç„¶æ”¶å°¾
5. 15åˆ†é’Ÿæ—¶ä¸»åŠ¨ã€è‡ªç„¶åœ°ç»“æŸé¢è¯•
6. å…¨ç¨‹ä¸è¦é€éœ²AIèº«ä»½ï¼Œåªæå‡ºé—®é¢˜å¹¶è¿½é—®

è¯·ä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­æ°”ï¼ŒæŠŠæ¡å¥½æ—¶é—´èŠ‚å¥ã€‚""",
    
    "pm": """ä½ æ˜¯äº§å“ç»ç†é¢è¯•å®˜ï¼Œæ­£åœ¨è¿›è¡Œä¸€åœº15åˆ†é’Ÿçš„ç»“æ„åŒ–é¢è¯•ã€‚
ä½ çš„ä»»åŠ¡ï¼š
1. å…ˆçƒ­æƒ…æ‰“æ‹›å‘¼å¹¶è¯´æ˜é¢è¯•æ—¶é•¿çº¦15åˆ†é’Ÿ
2. æ ¹æ®å€™é€‰äººçš„å›ç­”åŠ¨æ€ç”Ÿæˆäº§å“ç›¸å…³é—®é¢˜
3. åœ¨å›ç­”ç®€çŸ­æˆ–æ¨¡ç³Šæ—¶é€‚å½“è¿½é—®
4. é¢è¯•è¿›è¡Œåˆ°14åˆ†é’Ÿæ—¶å¼€å§‹è‡ªç„¶æ”¶å°¾
5. 15åˆ†é’Ÿæ—¶ä¸»åŠ¨ã€è‡ªç„¶åœ°ç»“æŸé¢è¯•
6. å…¨ç¨‹ä¸è¦é€éœ²AIèº«ä»½ï¼Œåªæå‡ºé—®é¢˜å¹¶è¿½é—®

è¯·ä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­æ°”ï¼ŒæŠŠæ¡å¥½æ—¶é—´èŠ‚å¥ã€‚""",
    
    "default": """ä½ æ˜¯ä¸“ä¸šçš„é¢è¯•å®˜ï¼Œæ­£åœ¨è¿›è¡Œä¸€åœº15åˆ†é’Ÿçš„ç»“æ„åŒ–é¢è¯•ã€‚
ä½ çš„ä»»åŠ¡ï¼š
1. å…ˆçƒ­æƒ…æ‰“æ‹›å‘¼å¹¶è¯´æ˜é¢è¯•æ—¶é•¿çº¦15åˆ†é’Ÿ
2. æ ¹æ®å€™é€‰äººçš„å›ç­”åŠ¨æ€ç”Ÿæˆé—®é¢˜
3. åœ¨å›ç­”ç®€çŸ­æˆ–æ¨¡ç³Šæ—¶é€‚å½“è¿½é—®
4. é¢è¯•è¿›è¡Œåˆ°14åˆ†é’Ÿæ—¶å¼€å§‹è‡ªç„¶æ”¶å°¾
5. 15åˆ†é’Ÿæ—¶ä¸»åŠ¨ã€è‡ªç„¶åœ°ç»“æŸé¢è¯•
6. å…¨ç¨‹ä¸è¦é€éœ²AIèº«ä»½ï¼Œåªæå‡ºé—®é¢˜å¹¶è¿½é—®

è¯·ä¿æŒä¸“ä¸šä½†å‹å¥½çš„è¯­æ°”ï¼ŒæŠŠæ¡å¥½æ—¶é—´èŠ‚å¥ã€‚"""
}

# æ¨¡å‹é…ç½®å­—å…¸
MODEL_CONFIGS = {
    "default": {
        "model": "deepseek-chat",
        "temperature": 0.7,
        "max_tokens": 2000,
        "top_p": 0.9
    },
    "structured": {
        "model": "deepseek-chat",
        "temperature": 0.6,  # é€‚åˆç»“æ„åŒ–é¢è¯•
        "max_tokens": 1800,
        "top_p": 0.85
    }
}

# å…¨å±€æ—¶é—´çŠ¶æ€
interview_start_time = None
TOTAL_INTERVIEW_SECONDS = 15 * 60  # 15åˆ†é’Ÿ
time_warnings_sent = []

def start_interview_timer():
    """å¼€å§‹é¢è¯•è®¡æ—¶"""
    global interview_start_time, time_warnings_sent
    interview_start_time = time.time()
    time_warnings_sent = []  # é‡ç½®è­¦å‘Šè®°å½•
    print(f"â° é¢è¯•è®¡æ—¶å¼€å§‹ï¼æ€»æ—¶é•¿ï¼š{TOTAL_INTERVIEW_SECONDS//60}åˆ†é’Ÿ")
    return interview_start_time

def get_interview_time_status():
    """è·å–é¢è¯•æ—¶é—´çŠ¶æ€"""
    if interview_start_time is None:
        return {"elapsed": 0, "remaining": TOTAL_INTERVIEW_SECONDS, "progress": 0}
    
    elapsed = time.time() - interview_start_time
    remaining = max(0, TOTAL_INTERVIEW_SECONDS - elapsed)
    progress = min(100, (elapsed / TOTAL_INTERVIEW_SECONDS) * 100)
    
    return {
        "elapsed": int(elapsed),
        "remaining": int(remaining),
        "progress": progress,
        "minutes_left": remaining // 60,
        "seconds_left": int(remaining % 60),
        "is_time_up": remaining <= 0
    }



def should_end_interview() -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸé¢è¯•"""
    status = get_interview_time_status()
    return status["is_time_up"]


def get_system_prompt(position_type: str) -> str:
    """æ ¹æ®å²—ä½ç±»å‹è·å–ç³»ç»Ÿæç¤ºè¯"""
    return SYSTEM_PROMPTS.get(position_type, SYSTEM_PROMPTS["default"])

def get_model_config(config_name: str) -> Dict:
    """è·å–æ¨¡å‹é…ç½®"""
    return MODEL_CONFIGS.get(config_name, MODEL_CONFIGS["default"])

def show_menu():
    """æ˜¾ç¤ºä¸»èœå•"""
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘       ğŸ¤– æ™ºèƒ½é¢è¯•ç³»ç»Ÿ v3.0        â•‘")
    print("â•‘    (15åˆ†é’Ÿç»“æ„åŒ–é¢è¯•-åŠ¨æ€é—®é¢˜)    â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    print("è¯·é€‰æ‹©é¢è¯•æ¨¡å¼ï¼š")
    print("  1ï¸âƒ£  15åˆ†é’Ÿç»“æ„åŒ–é¢è¯•")
    print("  0ï¸âƒ£  é€€å‡ºç³»ç»Ÿ")
    print()

def show_position_menu():
    """æ˜¾ç¤ºå²—ä½é€‰æ‹©èœå•"""
    print("\nè¯·é€‰æ‹©é¢è¯•å²—ä½ç±»å‹ï¼š")
    print("  1. é‡‘èç±» (æŠ•èµ„ã€é“¶è¡Œã€é£æ§)")
    print("  2. æŠ€æœ¯ç±» (å¼€å‘ã€ç®—æ³•ã€è¿ç»´)")
    print("  3. äº§å“ç±» (äº§å“ç»ç†ã€è¿è¥)")
    print("  4. è‡ªå®šä¹‰ (å…¶ä»–å²—ä½)")
    print()

def fifteen_minute_interview(position_name: str, system_prompt: str, model_config: Dict):
    """
    15åˆ†é’Ÿç»“æ„åŒ–é¢è¯•
    AIåŠ¨æ€ç”Ÿæˆé—®é¢˜ï¼Œä¸»åŠ¨æ§åˆ¶æ—¶é—´
    """
    # 1. å¼€å§‹è®¡æ—¶
    start_interview_timer()
    
    # 2. åˆå§‹åŒ–å†å²è®°å½•
    history = [{"role": "system", "content": system_prompt}]
    scores = []
    question_count = 0
    
    print(f"\nğŸ¯ {position_name} é¢è¯•å¼€å§‹ï¼")
    print("â° æ—¶é•¿ï¼š15åˆ†é’Ÿ")
    print("ğŸ¤– AIå°†åŠ¨æ€ç”Ÿæˆé—®é¢˜å¹¶æ§åˆ¶æ—¶é—´èŠ‚å¥")
    print("-" * 50)
    
    # 3. AIç”Ÿæˆå¼€åœºç™½ï¼ˆä¸å›ºå®šï¼‰
    print("ğŸ¤– é¢è¯•å®˜: ", end="", flush=True)
    
    opening_response = ""
    for chunk in llm_stream_chat(
        history=history,
        user_input="è¯·å¼€å§‹é¢è¯•ï¼Œå…ˆæ‰“æ‹›å‘¼å¹¶è¯´æ˜é¢è¯•æ—¶é•¿ã€‚",
        system_prompt="",
        model_name=model_config["model"],
        temperature=model_config["temperature"],
        max_tokens=model_config["max_tokens"],
        top_p=model_config["top_p"]
    ):
        print(chunk, end="", flush=True)
        opening_response += chunk
    
    print()  # æ¢è¡Œ
    history.append({"role": "assistant", "content": opening_response})
    
    # 4. ä¸»å¯¹è¯å¾ªç¯
    while True:
        try:
            # æ£€æŸ¥æ—¶é—´
            time_status = get_interview_time_status()
            
            # æ—¶é—´åˆ°äº†ï¼Œä¸»åŠ¨ç»“æŸ
            if time_status["is_time_up"]:
                print("\n" + "="*50)
                print("â° æ—¶é—´åˆ°ï¼")
                print("="*50)
                
                # AIç”Ÿæˆç»“æŸè¯­
                print("ğŸ¤– é¢è¯•å®˜: ", end="", flush=True)
                ending_response = ""
                for chunk in llm_stream_chat(
                    history=history,
                    user_input="é¢è¯•æ—¶é—´å·²åˆ°ï¼Œè¯·è‡ªç„¶åœ°ç»“æŸé¢è¯•ï¼Œæ„Ÿè°¢å€™é€‰äººå¹¶é€‚å½“æ€»ç»“ä»–åˆšåˆšçš„è¡¨ç°ã€‚",
                    system_prompt="",
                    model_name=model_config["model"],
                    temperature=model_config["temperature"],
                    max_tokens=800,  # ç»“æŸè¯­çŸ­ä¸€ç‚¹
                    top_p=model_config["top_p"]
                ):
                    print(chunk, end="", flush=True)
                    ending_response += chunk
                
                print()
                break
            
            
            
            
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            
            # é€€å‡ºæ£€æŸ¥
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'ç»“æŸ']:
                print("\nâ¹ï¸ é¢è¯•æå‰ç»“æŸ")
                break
            
            # è¯„ä¼°å‘½ä»¤
            if user_input.lower() == 'è¯„ä¼°':
                if scores:
                    avg = sum(scores) / len(scores)
                    print(f"ğŸ“Š å½“å‰å¹³å‡åˆ†: {avg:.1f}/5.0 (å…±{len(scores)}ä¸ªè¯„åˆ†)")
                else:
                    print("ğŸ“Š æš‚æ— è¯„åˆ†æ•°æ®")
                continue
            
            # å‡†å¤‡ç»™AIçš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å‰©ä½™æ—¶é—´ä¿¡æ¯ï¼‰
            remaining_minutes = time_status["minutes_left"]
            remaining_seconds = time_status["seconds_left"]
            
            # æ ¹æ®å‰©ä½™æ—¶é—´è°ƒæ•´æç¤º
            time_hint = ""
            if remaining_minutes == 14:
                time_hint = "ï¼ˆåˆšå¼€å§‹ï¼‰"
            elif remaining_minutes <= 10 and remaining_minutes > 5:
                time_hint = "ï¼ˆè¿›è¡Œä¸­ï¼‰"
            elif remaining_minutes <= 5 and remaining_minutes > 2:
                time_hint = "ï¼ˆæ·±å…¥è®¨è®ºï¼‰"
            elif remaining_minutes <= 2:
                time_hint = "ï¼ˆå‡†å¤‡æ”¶å°¾ï¼‰"
            
            # ç»„åˆè¾“å…¥
            enhanced_input = user_input
            if time_hint:
                enhanced_input = f"{user_input} {time_hint}"
            
            # AIç”Ÿæˆå›åº”
            print("ğŸ¤– é¢è¯•å®˜: ", end="", flush=True)
            
            full_response = ""
            for chunk in llm_stream_chat(
                history=history,
                user_input=enhanced_input,
                system_prompt="",
                model_name=model_config["model"],
                temperature=model_config["temperature"],
                max_tokens=model_config["max_tokens"],
                top_p=model_config["top_p"]
            ):
                print(chunk, end="", flush=True)
                full_response += chunk
            
            print()  # æ¢è¡Œ
            
            # æ›´æ–°å†å²è®°å½•
            history.append({"role": "user", "content": user_input})
            history.append({"role": "assistant", "content": full_response})
            
            # è‡ªåŠ¨è¯„ä¼°å›ç­”
            if len(history) > 3:
                # æ‰¾åˆ°æœ€è¿‘çš„é—®é¢˜
                recent_question = ""
                for msg in reversed(history[:-2]):
                    if msg["role"] == "assistant":
                        recent_question = msg["content"]
                        break
                
                if recent_question and len(user_input) > 15:
                    evaluation = evaluate_answer(recent_question, user_input)
                    scores.append(evaluation["score"])
            
            question_count += 1
            
            # æ§åˆ¶å†å²é•¿åº¦
            if len(history) > 12:
                history = [history[0]] + history[-10:]
            
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸ é¢è¯•ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            continue
    
    # 5. é¢è¯•ç»“æŸ
    print("\n" + "="*50)
    print("ğŸ¯ é¢è¯•ç»“æŸ")
    
    # è®¡ç®—å®é™…ç”¨æ—¶
    actual_seconds = time.time() - interview_start_time if interview_start_time else 0
    minutes = int(actual_seconds // 60)
    seconds = int(actual_seconds % 60)
    
    print(f"â±ï¸ å®é™…ç”¨æ—¶: {minutes}åˆ†{seconds}ç§’")
    print(f"ğŸ“Š æ€»é—®é¢˜æ•°: {question_count}")
    
    if scores:
        avg_score = sum(scores) / len(scores)
        print(f"â­ å¹³å‡è¯„åˆ†: {avg_score:.1f}/5.0")
    
    print("="*50)
    
    return question_count, scores

def main():
    """ä¸»å‡½æ•°ï¼šæ§åˆ¶æ•´ä¸ªé¢è¯•æµç¨‹"""
    
    while True:
        show_menu()
        choice = input("è¯·è¾“å…¥é€‰æ‹© (0-1): ").strip()
        
        if choice == '0':
            print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            break
        
        elif choice == '1':
            # 15åˆ†é’Ÿç»“æ„åŒ–é¢è¯•
            
            # 1. é€‰æ‹©å²—ä½ç±»å‹
            show_position_menu()
            pos_choice = input("è¯·é€‰æ‹©å²—ä½ç±»å‹ (1-4): ").strip()
            
            if pos_choice == '1':
                position_type = "finance"
                default_name = "é‡‘èåˆ†æå¸ˆ"
            elif pos_choice == '2':
                position_type = "tech"
                default_name = "è½¯ä»¶å·¥ç¨‹å¸ˆ"
            elif pos_choice == '3':
                position_type = "pm"
                default_name = "äº§å“ç»ç†"
            elif pos_choice == '4':
                position_type = "default"
                default_name = "å€™é€‰äºº"
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
                position_type = "default"
                default_name = "å€™é€‰äºº"
            
            # è¾“å…¥å…·ä½“å²—ä½åç§°
            position_name = input(f"è¯·è¾“å…¥å…·ä½“å²—ä½åç§° (å›è½¦ä½¿ç”¨ '{default_name}'): ").strip()
            if not position_name:
                position_name = default_name
            
            # 2. ä½¿ç”¨ç»“æ„åŒ–é¢è¯•ä¸“ç”¨é…ç½®
            model_config = get_model_config("structured")
            
            # 3. è·å–ç³»ç»Ÿæç¤ºè¯
            system_prompt = get_system_prompt(position_type)
            
            # 4. å¼€å§‹15åˆ†é’Ÿé¢è¯•
            question_count, scores = fifteen_minute_interview(position_name, system_prompt, model_config)
            
            # 5. ç”Ÿæˆæ€»ç»“
            if question_count > 0:
                summary = get_summary(position_name, question_count, scores)
                print("\n" + "="*50)
                print("ğŸ“ é¢è¯•æ€»ç»“æŠ¥å‘Š")
                print("="*50)
                print(summary)
                print("="*50)
            
            input("\næŒ‰å›è½¦é”®è¿”å›ä¸»èœå•...")
        
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

if __name__ == "__main__":
    main()
