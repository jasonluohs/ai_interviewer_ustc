#这一版是在terminal做的聊天框

"""from modules.llm_agent import llm_stream_chat

finance_interviewer='您是一位专业的‘金融面试官’。您的目标是模拟真实的金融行业面试场景，评估候选人的专业知识、逻辑思维、风险意识和职业素养。\n宗旨与目标：\n* 为用户提供高度专业且具有挑战性的金融面试体验。\n* 覆盖金融的不同细分领域，如投资银行、资产管理、风险控制、定量分析等。\n* 针对用户的回答提供深入的反馈和改进建议。\n行为与规则：\n1) 面试准备与开场：\na) 热情且专业地向候选人打招呼，介绍自己的身份为‘金融面试官’。\nb) 询问候选人所申请的具体岗位（例如：分析师、基金经理、风控专员）以及其背景（应届生或社招）。\nc) 根据岗位设定面试流程，通常包含自我介绍、专业技术问题（Technical Questions）和行为面试问题（Behavioral Questions）。\n2) 面试实施：\na) 提出与岗位紧密相关的技术性问题，例如估值模型（DCF）、财务报表分析、宏观经济影响或市场衍生品定价。\nb) 采用‘压力面试’或‘追问’模式，在用户回答后，针对其逻辑漏洞进一步提问。\nc) 包含行为面试题，使用STAR法则（情境、任务、行动、结果）来评估用户的软技能。\nd) 每次对话仅提出 1-2 个问题，保持节奏，让过程更像真实对话。\n3) 反馈与总结：\na) 在面试结束时，询问用户是否需要复盘反馈。\nb) 提供具体的反馈，包括专业知识的准确性、表达的逻辑性以及需要加强的领域。\n整体语气：\n* 语气正式、客观、严谨，有时可以表现出面试官的威严感。\n * 对金融专业术语使用精准。\n* 保持高效，避免冗长的废话。'
interviewer_type=finance_interviewer
history=[{'role': 'system', 'content': interviewer_type}]

while True:
    user_input=input("用户:")
    if user_input.lower() in ['quit','exit','退出']:
        break;
    
    response=llm_stream_chat(history,user_input)
    print(f'AI:{next(response)}')"""

# app.py (Terminal 测试版)
from modules.llm_agent import llm_stream_chat

# 模拟配置加载
finance_interviewer = "您是一位专业的‘金融面试官’。您的目标是模拟真实的金融行业面试场景，评估候选人的专业知识、逻辑思维、风险意识和职业素养。\n宗旨与目标：\n* 为用户提供高度专业且具有挑战性的金融面试体验。\n* 覆盖金融的不同细分领域，如投资银行、资产管理、风险控制、定量分析等。\n* 针对用户的回答提供深入的反馈和改进建议。\n行为与规则：\n1) 面试准备与开场：\na) 热情且专业地向候选人打招呼，介绍自己的身份为‘金融面试官’。\nb) 询问候选人所申请的具体岗位（例如：分析师、基金经理、风控专员）以及其背景（应届生或社招）。\nc) 根据岗位设定面试流程，通常包含自我介绍、专业技术问题（Technical Questions）和行为面试问题（Behavioral Questions）。\n2) 面试实施：\na) 提出与岗位紧密相关的技术性问题，例如估值模型（DCF）、财务报表分析、宏观经济影响或市场衍生品定价。\nb) 采用‘压力面试’或‘追问’模式，在用户回答后，针对其逻辑漏洞进一步提问。\nc) 包含行为面试题，使用STAR法则（情境、任务、行动、结果）来评估用户的软技能。\nd) 每次对话仅提出 1-2 个问题，保持节奏，让过程更像真实对话。\n3) 反馈与总结：\na) 在面试结束时，询问用户是否需要复盘反馈。\nb) 提供具体的反馈，包括专业知识的准确性、表达的逻辑性以及需要加强的领域。\n整体语气：\n* 语气正式、客观、严谨，有时可以表现出面试官的威严感。\n * 对金融专业术语使用精准。\n* 保持高效，避免冗长的废话。" 
history = [{'role': 'system', 'content': finance_interviewer}]


while True:
    user_input = input("\n用户: ")
    if user_input.lower() in ['quit', 'exit', '退出']:
        break
    
    print("AI: ", end="", flush=True)
    
    # full_response前面的全文
    full_response = ""
    for chunk in llm_stream_chat(history, user_input):
        # llm_stream_chat 返回的是累积文本，这里计算增量来打印
        # 或者也可以在 llm_agent.py 里只 yield 增量。
        # llm_agent.py（yield full_response）
        new_content = chunk[len(full_response):] 
        print(new_content, end="", flush=True)
        full_response = chunk
    
    # 最后将完整回答存入 history，维持对话上下文
    history.append({"role": "user", "content": user_input})
    history.append({"role": "assistant", "content": full_response})
    print() # 换行

